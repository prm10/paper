# 摘要
　　长久以来，由于围棋的搜索空间巨大、很难对位置和移动进行准确评分，围棋一直被视为人工智能领域最具挑战的经典游戏。这里我们介绍一种新方法来计算围棋，它使用值网络来评价位置，策略网络来决定移动方式。这些神经网络基于人类选手比赛数据进行有监督学习。并通过跟自己下棋来强化学习。这种神经网络不需要任何前向搜索就可以达到蒙特卡洛树搜索模拟上千盘比赛达到的最好效果。我们也介绍了一种融合蒙特卡洛方法和值网络、策略网络的新算法。使用这种算法，我们的AlphaGo以99.8%的胜率击败了其他围棋程序，还以5:0的总比分击败了人类的欧洲围棋冠军。这是迄今为止电脑程序第一次击败人类职业围棋选手，以前被认为至少是十年后的事。
# 简介
>s为状态，也就是棋谱
>b是宽度，也就是每一步可以走的地方
>d是深度
>则总共可以有约$b^d$种下法
>而在每种下法的每一步，都需要递归的计算最优化函数$v^*(s)$来决定下一步怎么走。

| 棋类 | b | d |
| ------------- |:-------------:| -----:|
| 国际象棋 | 35 | 80 |
| 围棋 | 250 | 150 |
　　显然，通过穷尽搜索空间来下围棋不现实，但是可以通过两种常用的理论来缩小需要搜索的空间。
　　最近，深度卷积神经网络在视觉领域取得了意料之外的成功。我们把这种结构用于围棋，我们将棋盘看成一个19*19的图像来形成对位置的表示。神经网络的作用一是用值网络来评价位置，二是用决策网络来采样决定下一步。
　　图1显示了我们算法流程：首先，直接利用人类的走法有监督学习一个策略网络$p_\sigma$，同时又跟已有工作类似地训练一个快速决策$p_\pi$，能够在训练的同时快速作出决策。然后，我们训练了一个强化学习决策网络$p_\rho$来通过优化最终的游戏输出来增强有监督学习网络，这使得决策朝着赢棋的方向调整，而不是最优化预测的准确率。最终，我们训练出了值网络$v_\theta$来预测有监督网络(RL)跟自己下棋时的胜利者。我们的方法能够跟已有的MCTS方法有机结合起来。
